# inference.py
dataset:
  dataset_names: 
    #- MMLU_biology
    #- MMLU_anatomy
    #- MMLU_medicine
    #- MMLU_clinical
    #- PubMedQA
    - MedMCQA
    #- HaluEval

model: ChatGLM

generation:
  few_shot: 0 # number of shots 
  CoT: False # True/ False 
  k_self_consistency: 0 # number of self-consistency
  top_p: 0.9
  temperature: 1
  batch_size: 3

# process_response.py
response:
  from_inference: True # set to true to use the responses by the same model with the same generation parameters in inference

  chosen_datasets: # Uncomment the datasets you would like to shorten and specify its corresponding path 
   MMLU_biology: "responses/{{ model }}/{{ prompt }}/MMLU_biology" 
   #MMLU_anatomy: "responses/{{ model }}/MMLU_anatomy" 
   #MMLU_medicine: "responses/{{ model }}/MMLU_medicine" 
   #MMLU_clinical: "responses/{{ model }}/MMLU_clinical"
   #PubMedQA: "responses/{{ model }}/PubMedQA"
   #MedMCQA: "responses/{{ model }}/MedMCQA"
   #HaluEval: "responses/{{ model }}/HaluEval"

  shortened_save_path: null

# eval.py
eval: shortened/Med42/0-shot
